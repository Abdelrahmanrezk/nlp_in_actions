{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Moved from vectors that represent the appearance or absence of a word in a document like one-hot, which actually have a great representation of document because it keeps the order of the words in the document which keep the grammar, but it consumes a lot to process even small corpus, also the count of a word in the document have a great turned this memory usage in One-hot to just represent the count of the word in the document, but we know that some words appear a lot than others.\n",
    "\n",
    "But now we need more information about the content that the words share in some document, like word frequency related to the other document, and to represent something like that we need continuous value to represent the word, then later we see how to represent these numbers in a way the represent not the importance of the word, but the meaning of the word it self.\n",
    "\n",
    "- TF-IDF: Term Frequency **Times** Inverse Document Frequency.\n",
    "\n",
    "We will see:\n",
    "\n",
    "- Bag-of-Words: Word counts or Frequency.\n",
    "- Bag-of-n-grams: Count of word pairs(Bigram) and 3-grams (triplets) and so on.\n",
    "- TF-IDF: Word score that better represents their **importance** not **meaning**.\n",
    "\n",
    "### *Lets go first in the first part **TF**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/abdelrahman/.local/lib/python3.6/site-packages/pugnlp/constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/abdelrahman/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "from nlpia.data.loaders import kite_text, kite_history\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import numpy as np\n",
    "import math\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\"\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 4,\n",
       "         'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = Counter(tokens)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('faster', 3), (',', 3), ('harry', 2), ('got', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.most_common(5) # method associated with counter class, return most repeated words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "We can not assume that words that appear more times are more important to document than others, as we can see some tokens like [the, ,] and for an application like sentiment analysis words like [better, good] does not mean that the document is positive, what about the intent of the meaning.\n",
    "\n",
    "Also using **python dictionary** or libraries like Collections, can shuffle the order of the word appearance, may be we can still capture the meaning after this shuffle but what about the long document.\n",
    "\n",
    "The length of the document relative to the TF (Term Frequency) of that word in the document is important, because of we have multiple documents, it happens to have the **same** word little than in other document but its more important, this is because of the length of the document.\n",
    "\n",
    "Suppose doc A have word **cat** appear 3 times in len doc of 30,\n",
    "and doc B have word also **cat** but appear 100 times in len of 580000.\n",
    "\n",
    "- Doc A is **email veterinarian**\n",
    "- Doc B is **War  & Peace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.00017\n"
     ]
    }
   ],
   "source": [
    "doc_a_cat = 3/30\n",
    "doc_b_cat = 100/580000\n",
    "print(doc_a_cat)\n",
    "print(round(doc_b_cat,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "\n",
    "So instead of **TF** (3 or 100 times) we can have actually the **importance of the word to document** using **Normalized Term Frequency** (.1 or .00017) frequence actually its probability of word not frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18182"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_harry_appears = bag_of_words['harry']\n",
    "num_unique_words    = len(bag_of_words)\n",
    "round(times_harry_appears/num_unique_words, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized TF\n",
    "\n",
    "By using Normalized TF, we can see that describes something about the two document and their relationship to word cat and we can do for any word !, so instead of just raw count (TF), normlized these counts give you relative information.\n",
    "\n",
    "So we can get each word and how it's relatively important to the document.\n",
    "\n",
    "Let's move to a bigger piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A kite is traditionally a tethered heavier-than-air craft with wing surfaces that react against the air to create lift and drag. A kite consists of wings, tethers, and anchors. Kites often have a bridle to guide the face of the kite at the correct angle so the wind can lift it. A kite's wing also may be so designed so a bridle is not needed; when kiting a sailplane for launch, the tether meets the wing at a single point. A kite may have fixed or moving anchors. Untraditionally in technical kiting, a kite consists of tether-set-coupled wing sets; even in technical kiting, though, a wing in the system is still often called the kite.\\n\\nThe lift that sustains the kite in flight is generated when air flows around the kite's surface, producing low pressure above and high pressure below the wings. The interaction with the wind also generates horizontal drag along the direction of the wind. The resultant force vector from the lift and drag force components is opposed by the tension of one or more of the lines or tethers to which the kite is attached. The anchor point of the kite line may be static or moving (e.g., the towing of a kite by a running person, boat, free-falling anchors as in paragliders and fugitive parakites or vehicle).\\n\\nThe same principles of fluid flow apply in liquids and kites are also used under water.\\n\\nA hybrid tethered craft comprising both a lighter-than-air balloon as well as a kite lifting surface is called a kytoon.\\n\\nKites have a long and varied history and many different types are flown individually and at festivals worldwide. Kites may be flown for recreation, art or other practical uses. Sport kites can be flown in aerial ballet, sometimes as part of a competition. Power kites are multi-line steerable kites designed to generate large forces which can be used to power activities such as kite surfing, kite landboarding, kite fishing, kite buggying and a new trend snow kiting. Even Man-lifting kites have been made.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets look at some words of Bag-of-words we have\n",
    "kite_text # Our Corups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(kite_text.lower()) # Tokenization\n",
    "bag_of_words = Counter(tokens) # Get unique words and their counts\n",
    "print(len(tokens))\n",
    "print(len(bag_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "16\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words['kite.'])\n",
    "print(bag_of_words['kite'])\n",
    "print(bag_of_words['water.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stope Words\n",
    "\n",
    "A lot of stop words we have andy maybe it does not add any information as the text is about articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/abdelrahman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "kite_counts = Counter(tokens)\n",
    "print(len(tokens))\n",
    "print(len(kite_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'kite': 16,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'react': 1,\n",
       "         'air': 2,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'wings': 1,\n",
       "         ',': 15,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'wind': 2,\n",
       "         'it.': 1,\n",
       "         \"'s\": 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'designed': 2,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'high': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'lines': 1,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'e.g.': 1,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'used': 2,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kite_counts # We can see the difference after remove stopwords how its reduction the size of Bag-of-word we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words['kite.'])\n",
    "print(bag_of_words['lift'])\n",
    "print(bag_of_words['wing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TreebankWordTokenizer\n",
    "\n",
    "It return \"kite.\" and \"kite\" as different tokens, it assume that your text segmented into sentences or documents, so it's only ignore punctuations at the very ending of strings when comes(\\n).\n",
    "\n",
    "sentences segmentation is tricky, we learn about later.\n",
    "\n",
    "Spacy is working better for now, because it is due to sentence segmentation and tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## From Text to numbers what next ?\n",
    "\n",
    "We have turned the textual data into numbers, but now instead of just count the word in the vector, we need values in the vector to be relative to something consistent across all vectors. For math operations is like to have the same scale(diemntion), share the same orgin.\n",
    "\n",
    "So we do:\n",
    "- Normalization\n",
    "- Share same length or diemntion of all vectors\n",
    "- want the value of each element of the vector to represent the same word in each document's Vector, so its fine to have 0 in some position of absence words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10884353741496598,\n",
       " 0.10204081632653061,\n",
       " 0.05442176870748299,\n",
       " 0.034013605442176874,\n",
       " 0.027210884353741496,\n",
       " 0.027210884353741496,\n",
       " 0.02040816326530612,\n",
       " 0.02040816326530612,\n",
       " 0.02040816326530612,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.013605442176870748,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374,\n",
       " 0.006802721088435374]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To Normalized TF\n",
    "\n",
    "normalized_tf = []\n",
    "for key, val in kite_counts.most_common():\n",
    "    normalized_tf.append(val/len(kite_counts))\n",
    "normalized_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "With just one vector for one document math does not have anything to do !, so lets us get other documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpia.data.loaders import harry_docs as docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The faster Harry got to the store, the faster and faster Harry would get home.',\n",
       " 'Harry is hairy and faster than Jill.',\n",
       " 'Jill is not as hairy as Harry.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "17\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "doc_tokens = []\n",
    "all_doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens    += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "    all_doc_tokens +=  sorted(tokenizer.tokenize(doc.lower()))\n",
    "print(len(doc_tokens))\n",
    "print(len(doc_tokens[0]))\n",
    "print(len(all_doc_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'as',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not',\n",
       " 'store',\n",
       " 'than',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words\n",
    "lexicon = sorted(set(all_doc_tokens))\n",
    "print(len(lexicon))\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again\n",
    "\n",
    "Now each of the documents will take the same shared length **18** and give 0 for word that absence in this document and Normalized Tf for appearance one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('got', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.05555555555555555),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.16666666666666666),\n",
       "              ('get', 0.05555555555555555),\n",
       "              ('got', 0.05555555555555555),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.1111111111111111),\n",
       "              ('home', 0.05555555555555555),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.05555555555555555),\n",
       "              ('than', 0),\n",
       "              ('the', 0.16666666666666666),\n",
       "              ('to', 0.05555555555555555),\n",
       "              ('would', 0.05555555555555555)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.05555555555555555),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.05555555555555555),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0.05555555555555555),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_count = Counter(tokens)\n",
    "    for key, val in token_count.items():\n",
    "        vec[key] = val / len(lexicon)\n",
    "    doc_vectors.append(vec)\n",
    "doc_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note !\n",
    "\n",
    "Now we have three vectors, one for each document and we can apply math operation, but lets take a time understand vectors and vectors space in math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space\n",
    "\n",
    "Vector is primary building block in the linear algebra, these vectors are list of numbers or cordinates in vector space, they decribe a location or position in that space or they can be used to identify a particular direction and magnitude (length) or distance in that space. A space is the collection of all posible vectors that could apear in that space. So vector with 2 space values would lie in 2-d vector space and so on.\n",
    "\n",
    "In NLP the space is all the unique words in our courps and more spaces means more computions power as when you search in list of 5 numbers not as in list of million number.\n",
    "\n",
    "To represent the space of our example which 18-d we can not do it so there is something will know about later called dimension reduction to reduce your 18-d to 2-d or 3-d that you can display.\n",
    "\n",
    "actually linear algebra work for 2-d space as same as any number of space but computation power be a large and distance between vectors be large.\n",
    "\n",
    "<img src='1.jpg'>\n",
    "\n",
    "As we can see we can do different math operations and equations to get the distance between two vectors and the angle between them in order to know the similarity.\n",
    "\n",
    "<img src='2.png'>\n",
    "\n",
    "It output range from -1 to 1 which represent how two vector are share same direction or not which in other word is these vector similar or not as we can see below.\n",
    "In NLP 1 means that the two vector share the same words and between that is the angle from -1 to 1 as we get.\n",
    "\n",
    "**when its 0 then it means no words are common between two vector, and this means they talking about completely different things and actually not means they have different meaning or topics, just they use differnt words.**\n",
    "\n",
    "It can not be -1 as we do not have TF of negative values, but in math it means they point in opposite dierctions, so in this case it will be in same space in case of 2-d same **quadrant**.\n",
    "\n",
    "Later we can see negative values when we develop a concept of words and topics that are **opposite** to each other, then we will see cosine of negative values. !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vec1, vec2):\n",
    "    result = ( np.matmul(vec1, vec2)) / (np.sum((np.linalg.norm(vec1))) * np.sum((np.linalg.norm(vec2))))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9679968981658008"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1 = np.array([.1, .17])\n",
    "vec2 = np.array([.056, .056])\n",
    "cosine_sim(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(vec1, vec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Zipfs Law\n",
    "\n",
    "From the pattern when display the rank in descending order showed that. The log frequency of any word is inversely proportional to its rank in the frequency table, means that first item will appear twice as often the second, and three times as often the third and so on in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see brown courps from nltk\n",
    "brown.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets remove the punctuations\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puncs = string.punctuation\n",
    "puncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034378"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = [x.lower() for x in brown.words() if x not in puncs]\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 69971),\n",
       " ('of', 36412),\n",
       " ('and', 28853),\n",
       " ('to', 26158),\n",
       " ('a', 23195),\n",
       " ('in', 21337),\n",
       " ('that', 10594),\n",
       " ('is', 10109),\n",
       " ('was', 9815),\n",
       " ('he', 9548)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "Return to TF and Normalized Term frequency that was normalized by the length of the Vocabulary size, is not tell us much more about the important of the word to a document that appear in **relative** to other documents rather than if we add the pattern of Zipfs law.\n",
    "\n",
    "If you could suss out that information , you could describe document within the corpus.  Some words as we can see like \"the\" are likely to appear accross all documents while \"construction\" or \"aerodynamics\" less likely to see in the documents, and for those ones where frequently occured we need to know more about the nature of document it contain, so for this we need IDF (inverse document frequently) which dervitive from Zipfs law in topic (document) analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363\n",
      "297\n"
     ]
    }
   ],
   "source": [
    "kite_intro = kite_text.lower()\n",
    "intro_tokens = tokenizer.tokenize(kite_intro)\n",
    "kite_history = kite_history.lower()\n",
    "history_tokens = tokenizer.tokenize(kite_history)\n",
    "intro_total = len(intro_tokens)\n",
    "print(intro_total)\n",
    "history_total = len(history_tokens)\n",
    "print(history_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf = Counter(intro_tokens)\n",
    "history_tf = Counter(history_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0440771349862259\n",
      "0.020202020202020204\n"
     ]
    }
   ],
   "source": [
    "print(intro_tf['kite'] / intro_total)\n",
    "print(history_tf['kite'] / history_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027548209366391185\n",
      "0.030303030303030304\n"
     ]
    }
   ],
   "source": [
    "print(intro_tf['and'] / intro_total)\n",
    "print(history_tf['and'] / history_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note !!\n",
    "\n",
    "\n",
    "We can see that how the word kite was appeared twice in kite_intro rather than in kite history means that if we have much more documents its appeared would be less likely, and on the other side the word \"and\" was appeared much more in second document rather than the first one !\n",
    "\n",
    "As we can see that we know can get more information about the important of word to the document as same to the other documents, because if it appear more in one documents but less likely in other documents means that its so important to this document on the other side if it much occurred across all documents like word \"and\", it will be less important to document.\n",
    "\n",
    "\n",
    "\n",
    "**IDF** the ratio of total number of documents to the number of documents the word (term) appear in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.010101010101010102\n"
     ]
    }
   ],
   "source": [
    "print(intro_tf['china'] / intro_total)\n",
    "print(history_tf['china'] / history_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets see and and china\n",
    "#### Total document 2\n",
    "#### and appears in the two document as we can see the normalized TF, so 2/2 = 1, two document/ appear in the two document\n",
    "#### china appears just 1 as we can see the normalized TF, so 2/1 = 2, two document/ appear in the one document\n",
    "\n",
    "** We can see who have  much weight than other **\n",
    "\n",
    "So Now we can conclude that:\n",
    "TF = Term frequency / document length.\n",
    "IDF = All docs / Docs contain this term (count one for each document it appears in).\n",
    "\n",
    "TF(t, d) = Count(t) / Count(d), \n",
    "- T is the word it self\n",
    "- d is the number of words in the document\n",
    "\n",
    "IDF(t, D) = log(number of documents / number of documents containing t)\n",
    "\n",
    "Now the TF-IDF is by myltiply the two result\n",
    "\n",
    "tf_idf(t,d, D) = tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF \n",
    "\n",
    "Lets look again step by step for tf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0440771349862259 0.020202020202020204\n",
      "==================================================\n",
      "0.027548209366391185 0.030303030303030304\n",
      "==================================================\n",
      "0.0 0.010101010101010102\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "intro_kite_tf = intro_tf['kite'] / intro_total # doc 1\n",
    "history_kite_tf = history_tf['kite'] / history_total # doc 2\n",
    "\n",
    "intro_and_tf = intro_tf['and'] / intro_total # doc 1\n",
    "history_and_tf = history_tf['and'] / history_total # doc 2\n",
    "\n",
    "intro_china_tf = intro_tf['china'] / intro_total # doc 1\n",
    "history_china_tf = history_tf['china'] / history_total # doc 2\n",
    "\n",
    "print(intro_kite_tf, history_kite_tf) \n",
    "print(\"=\"*50)\n",
    "print(intro_and_tf, history_and_tf)\n",
    "print(\"=\"*50)\n",
    "print(intro_china_tf, history_china_tf)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDF \n",
    "\n",
    "Lets look again step by step for IDF.\n",
    "\n",
    "We have a constant which is number of documents.\n",
    "\n",
    "From above we can see that each print have two real numbers exept last one first number is 0 which means that the word not appeared in the first document, so we will devide by 2 for each word except last one as we just have 2 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "doc_number = 2\n",
    "\n",
    "kite_idf = 2 / 2 # as it appeared in the two documents\n",
    "\n",
    "and_idf = 2 / 2 # as it appeared in the two documents\n",
    "\n",
    "china_idf = 2 / 1 # as it appeared in the just one document ????????\n",
    "\n",
    "print(kite_idf)\n",
    "print(and_idf)\n",
    "print(china_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The important of Kite in intro  document and accross other documents 0.0440771349862259\n",
      "The important of Kite in history document  and accross other documents 0.020202020202020204\n",
      "The important of and in intro  document and accross other documents 0.027548209366391185\n",
      "The important of and in history document  and accross other documents 0.030303030303030304\n",
      "The important of china in intro  document and accross other documents 0.0\n",
      "The important of china in history document  and accross other documents 0.020202020202020204\n"
     ]
    }
   ],
   "source": [
    "print(\"The important of Kite in intro  document and accross other documents\", (intro_kite_tf * kite_idf))\n",
    "print(\"The important of Kite in history document  and accross other documents\", (history_kite_tf * kite_idf))\n",
    "\n",
    "print(\"The important of and in intro  document and accross other documents\", (intro_and_tf * and_idf))\n",
    "print(\"The important of and in history document  and accross other documents\", (history_and_tf * and_idf))\n",
    "\n",
    "print(\"The important of china in intro  document and accross other documents\", (intro_china_tf * china_idf))\n",
    "print(\"The important of china in history document  and accross other documents\", (history_china_tf * china_idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "So the 0 in the fifth print is about that the word china actually does not appear in this document, so it has no weights.\n",
    "\n",
    "if we have more documents we will see how other words are important to documents even they appear less likely than other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return to Zipfs\n",
    "\n",
    "We have seen that in brown corpus as it semi large dataset we have the words appears large and large, and some words on the other side will just seen in small number of documents maybe 10 of million documents and other word just appeared 1, and in this case you will have idf = 1000,000 / 10 = 1000,00 and 1000,000 / 1 = 1000,000, even of that is just the distance is just in 10 documents but it exponentially higher frequency, so Sipfs suggest use the log which the inverse of exponential to return to the rigth state.\n",
    "\n",
    "This ensure that TF-IDF scores are more uniformly distributed, so we can now define the IDF as the log of the original probability of that word occuring in one of your documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Ranking\n",
    "\n",
    "Before we use word count as our main feature and replace the word in each document with this count, but it does not give us more information about the important of this word to the document or topic it talks about. But now after some analysis and Zipf's law we have noticed some pattern to keeping going improve the information we get about the word it self and how its effect on the document not just this but also relevant to other documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "documt_tfidf_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    \n",
    "    for key, val in token_counts.items():\n",
    "        docs_contain_key = 0\n",
    "        for _doc in docs:\n",
    "            if key in _doc:\n",
    "                docs_contain_key +=1\n",
    "        tf = val / len(lexicon)\n",
    "        if docs_contain_key:\n",
    "            idf = len(docs) / docs_contain_key\n",
    "        else: idf = 0\n",
    "        \n",
    "        vec[key] = tf * idf\n",
    "    documt_tfidf_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0.16666666666666666),\n",
       "             ('.', 0.05555555555555555),\n",
       "             ('and', 0.08333333333333333),\n",
       "             ('as', 0),\n",
       "             ('faster', 0.25),\n",
       "             ('get', 0.16666666666666666),\n",
       "             ('got', 0.16666666666666666),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0.0),\n",
       "             ('home', 0.16666666666666666),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0.16666666666666666),\n",
       "             ('than', 0),\n",
       "             ('the', 0.5),\n",
       "             ('to', 0.16666666666666666),\n",
       "             ('would', 0.16666666666666666)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documt_tfidf_vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity\n",
    "\n",
    "Now you can get the angle between two vectors to know they similar or not at all, or even use it to check query on your documents and return the most similar documents based on the query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How long does not it take as to get to the store?\"\n",
    "query_vec = copy.copy(zero_vector)\n",
    "tokens = tokenizer.tokenize(query.lower())\n",
    "token_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in token_counts.items():\n",
    "    docs_contain_key = 0\n",
    "    for _doc in docs:\n",
    "        if key in _doc.lower():\n",
    "            docs_contain_key +=1\n",
    "    if docs_contain_key == 0:\n",
    "        continue\n",
    "    tf = val / len(tokens)\n",
    "    idf = len(docs) / docs_contain_key\n",
    "    query_vec[key] = tf * idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5697334438987307"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = np.array(list(query_vec.values()))\n",
    "# q = q.reshape(-1, 1)\n",
    "doc1 = np.array(list(documt_tfidf_vectors[0].values()))\n",
    "doc2 = np.array(list(documt_tfidf_vectors[1].values()))\n",
    "doc3 = np.array(list(documt_tfidf_vectors[2].values()))\n",
    "# doc1 = doc1.reshape(-1,1)\n",
    "cosine_sim(q, doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(q, doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2993266539431952"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(q, doc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools\n",
    "Instead of doing hard code for everything, a lot of APIs are available for us to use. But now we have the intuation beside of what we will use we know hot it works.\n",
    "\n",
    "TfidfVectorizer return a sparce materix(most of 0) because we know each document is small number of words of the whole corpus.\n",
    "\n",
    "Using **todense** convert the sparce metrix to numbey metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16 0.   0.48 0.21 0.21 0.   0.25 0.21 0.   0.   0.   0.21 0.   0.64\n",
      "  0.21 0.21]\n",
      " [0.37 0.   0.37 0.   0.   0.37 0.29 0.   0.37 0.37 0.   0.   0.49 0.\n",
      "  0.   0.  ]\n",
      " [0.   0.75 0.   0.   0.   0.29 0.22 0.   0.29 0.29 0.38 0.   0.   0.\n",
      "  0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "corpus = docs\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "print(model.todense().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn\n",
    "\n",
    "** As we can see how it simple using sklearn instead of all of the code above**\n",
    "\n",
    "TfidfVectorizer have its own tokenizer so it maybe different from what we hand code, but now you have a matrix that represent each document, TF-IDF for each term in your lexicon(unique words)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
